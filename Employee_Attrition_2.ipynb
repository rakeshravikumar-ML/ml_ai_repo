{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e1e6a1",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We need several libraries to handle data, visualize trends, and build machine learning models.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Ignore harmless warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "```\n",
    "\n",
    "### **Breaking Down Each Import:**\n",
    "1. **NumPy (`numpy`)**:  \n",
    "   - Used for numerical computations.\n",
    "   - Helps in handling arrays and performing mathematical operations.\n",
    "\n",
    "2. **Pandas (`pandas`)**:  \n",
    "   - A powerful library for handling structured data.\n",
    "   - Used for reading, transforming, and analyzing the dataset.\n",
    "\n",
    "3. **Matplotlib (`matplotlib.pyplot`)**:  \n",
    "   - Provides basic plotting functions.\n",
    "   - Used to **visualize trends in employee attrition**.\n",
    "\n",
    "4. **Seaborn (`seaborn`)**:  \n",
    "   - Built on Matplotlib but provides **better statistical visualizations**.\n",
    "   - Helps in **heatmaps, boxplots, and trend analysis**.\n",
    "\n",
    "5. **Warnings (`warnings.filterwarnings('ignore')`)**:  \n",
    "   - Suppresses unnecessary warnings for cleaner outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dcd22",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Load the Employee Attrition Dataset\n",
    "\n",
    "We load the dataset using **Pandas** and inspect its structure.\n",
    "\n",
    "```python\n",
    "data = pd.read_csv('/content/drive/MyDrive/Datasets/Input/employee_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "1. **`pd.read_csv()`**:  \n",
    "   - Reads the dataset from a CSV file.\n",
    "   - Stores it in a Pandas **DataFrame**.\n",
    "\n",
    "2. **`data.head()`**:  \n",
    "   - Displays the **first five rows** of the dataset.\n",
    "   - Helps us **quickly inspect** its structure.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- We need to understand the **features** available before performing attrition analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import\n",
    "data = pd.read_csv('/content/drive/MyDrive/Datasets/Input/employee_data.csv')\n",
    "# Data sample\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203225a",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before building a model, we **explore** the dataset:\n",
    "\n",
    "```python\n",
    "print(f'The dataset has {data.shape[0]} rows and {data.shape[1]} columns.')\n",
    "data.info()\n",
    "data.describe()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`data.shape`**:  \n",
    "   - Returns the **number of rows and columns**.\n",
    "   - Helps understand the dataset **size**.\n",
    "\n",
    "2. **`data.info()`**:  \n",
    "   - Displays column names, **data types**, and **missing values**.\n",
    "\n",
    "3. **`data.describe()`**:  \n",
    "   - Shows **summary statistics** (mean, std, min, max, etc.).\n",
    "\n",
    "### **Why is this Important?**\n",
    "- Helps detect **data quality issues** before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns in the data\n",
    "rows, cols = data.shape\n",
    "print(f'The data has {rows} rows and {cols} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60977b72",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Checking for Missing Values\n",
    "\n",
    "Missing values can affect model accuracy. We count missing values in each column.\n",
    "\n",
    "```python\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values[missing_values > 0]\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`data.isnull().sum()`**:  \n",
    "   - Counts **missing values** in each column.\n",
    "\n",
    "2. **`missing_values[missing_values > 0]`**:  \n",
    "   - Filters out only columns that **have missing values**.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- Missing data can **bias** the analysis.\n",
    "- We decide whether to **drop or impute** missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numerical and categorical features\n",
    "num, obj = 0, 0\n",
    "for feature in data.columns:\n",
    "    if data[feature].dtype == 'O':\n",
    "        obj += 1\n",
    "    else:\n",
    "        num += 1\n",
    "print('NUMBER OF CATEGORICAL AND NUMERICAL FEATURES:')\n",
    "print(f'The data has {obj} categorical and {num} numerical features.')\n",
    "\n",
    "# Percentage of missing values\n",
    "print('\\nPERCENTAGE OF MISSING VALUES:')\n",
    "total = 0\n",
    "for feature in data.columns:\n",
    "    total += len(data[feature])\n",
    "\n",
    "missing = round(data.isnull().mean()*100,2)\n",
    "print('There is no missing value in dataset.' if total == data.size else missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e9abc",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Attrition Rate Analysis\n",
    "\n",
    "We analyze how many employees **left** vs. **stayed**.\n",
    "\n",
    "```python\n",
    "sns.countplot(x='Attrition', data=data, palette='coolwarm')\n",
    "plt.title('Employee Attrition Distribution')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`sns.countplot(x='Attrition', data=data, palette='coolwarm')`**:  \n",
    "   - Creates a **count plot** showing attrition distribution.\n",
    "   - **X-axis** = Attrition categories (Yes/No).\n",
    "\n",
    "2. **`plt.title()` & `plt.show()`**:  \n",
    "   - Adds a title & displays the plot.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- Employee attrition datasets are often **imbalanced** (fewer employees leaving than staying).\n",
    "- Class imbalance can **affect model predictions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fde618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data duplicates\n",
    "rows, cols = data[data.duplicated()].shape\n",
    "print('There are no duplicates.' if rows == 0 else f'There are {rows} duplicates in the data.')\n",
    "\n",
    "# if you find any duplicates then treat it\n",
    "    # data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81128e",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Correlation Analysis\n",
    "\n",
    "We check how numerical features are related.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(data.corr(), cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`data.corr()`**:  \n",
    "   - Computes Pearson correlation between numerical columns.\n",
    "\n",
    "2. **`sns.heatmap()`**:  \n",
    "   - Creates a **heatmap** for correlation visualization.\n",
    "\n",
    "3. **`cmap='coolwarm', annot=True`**:  \n",
    "   - Uses **coolwarm color scale** & shows values inside cells.\n",
    "\n",
    "### **Why is this Important?**\n",
    "- Helps identify **highly correlated features** (which may be redundant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature \"Over18\" is populated with one value, it wont contribute to analysis\n",
    "del data['Over18']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bbdedd",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Feature Engineering - Encoding Categorical Variables\n",
    "\n",
    "Machine learning models cannot process **categorical variables** directly.  \n",
    "We convert them into **numerical format** using:\n",
    "\n",
    "1. **Label Encoding** (For binary categorical features).  \n",
    "2. **One-Hot Encoding** (For multi-category features).  \n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode binary categorical variables (Yes/No â†’ 0/1)\n",
    "binary_cols = ['Attrition', 'OverTime']\n",
    "for col in binary_cols:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# One-Hot Encoding for categorical features with multiple categories\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "print(\"Encoded dataset preview:\")\n",
    "data.head()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`LabelEncoder().fit_transform(data[col])`**:  \n",
    "   - Converts **Yes/No columns** into **0/1** format.\n",
    "\n",
    "2. **`pd.get_dummies(data, drop_first=True)`**:  \n",
    "   - Performs **One-Hot Encoding**, creating new **binary columns** for each category.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Models **cannot handle text**; categorical data must be converted.\n",
    "- Encoding ensures that the model **understands feature relationships**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the numerical features\n",
    "for feature in data.columns:\n",
    "    if data[feature].dtype != 'O':\n",
    "        if len(data[feature].unique()) == 1:\n",
    "            print(f'** {feature} has {len(data[feature].unique())} unique values **')\n",
    "        else:\n",
    "            print(f'{feature} has {len(data[feature].unique())} unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d8727",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Handling Class Imbalance\n",
    "\n",
    "If the dataset has **more employees staying** than leaving,  \n",
    "the model may **struggle to predict attrition correctly**.\n",
    "\n",
    "We handle imbalance using **Synthetic Minority Over-sampling Technique (SMOTE)**.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['Attrition'])\n",
    "y = data['Attrition']\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Class distribution after SMOTE:\", y_resampled.value_counts())\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`SMOTE(sampling_strategy='auto')`**:  \n",
    "   - Generates **synthetic samples** for the minority class.  \n",
    "\n",
    "2. **`fit_resample(X, y)`**:  \n",
    "   - Balances the dataset by **over-sampling the minority class**.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Prevents the model from being **biased towards majority class**.\n",
    "- Ensures **fair prediction of attrition**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2dea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature \"StandardHours\" and \"EmployeeCount\" has only one unique value\n",
    "# Feature \"EmployeeNumber\" look like a ID column\n",
    "\n",
    "for cols in ['StandardHours', 'EmployeeNumber', 'EmployeeCount']:\n",
    "    del data[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5412f8",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9: Model Selection and Training\n",
    "\n",
    "We train a **Logistic Regression model** to predict attrition.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)`**:  \n",
    "   - Splits the dataset into **80% training** and **20% testing**.\n",
    "\n",
    "2. **`LogisticRegression()`**:  \n",
    "   - A statistical model used for **binary classification**.\n",
    "\n",
    "3. **`model.fit(X_train, y_train)`**:  \n",
    "   - Trains the model using the **training data**.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Helps predict whether an employee **will leave or stay**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ec915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION ON QUALITATIVE AND QUANTITATIVE DATA DISTRIBUTION UPON ATTRITION:\n",
    "\n",
    "\n",
    "#   1. For the quantitative variables, the distribution of attition 'yes' folows the same pattern of distribution\n",
    "#      for attrition 'No' with less density.\n",
    "\n",
    "#   2. For the qualtitative variables, the count of atrition 'yes' looks like a scaled down count of attrition\n",
    "#      'No' except that of JobRole, MaritalStatus and OverTime.\n",
    "\n",
    "#   3. Since the distribution of attrition follows a similar pattern with less density, further analysis\n",
    "#      can be carried on with data where attrition is 'Yes'.\n",
    "\n",
    "#   4. Similarly for qualitative variables, the attrition follows a scaled down count, further analysis\n",
    "#      can be carried on with data where attrition is 'Yes'.Excpet JobRole, MaritalStatus and OverTime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7eadc4",
   "metadata": {},
   "source": [
    "\n",
    "## Step 10: Model Evaluation - Precision-Recall Curve\n",
    "\n",
    "Since attrition is **imbalanced**, we use the **Precision-Recall Curve** to evaluate performance.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get model probabilities\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Plot the curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, marker='.', label=\"Logistic Regression\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`model.predict_proba(X_test)[:, 1]`**:  \n",
    "   - Gets the **probability scores** for class **1 (Attrition)**.\n",
    "\n",
    "2. **`precision_recall_curve(y_test, y_scores)`**:  \n",
    "   - Computes the **Precision-Recall curve**.\n",
    "\n",
    "3. **`plt.plot(recall, precision, marker='.')`**:  \n",
    "   - Plots the **Precision vs Recall**.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Traditional accuracy is **misleading for imbalanced datasets**.\n",
    "- Precision-Recall Curve helps us **better assess performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION:\n",
    "\n",
    "#    1. Employees who are male are most likely to attrite.\n",
    "#    2. Employees from R&D and sales department are most like to attrite.\n",
    "#    3. Employees from life science and medical background are most likely to attrite.\n",
    "#    4. Employees working as laboratory technician,sales executive, reasearch scientist,\n",
    "#       sales representative are most likely to attrite.\n",
    "#    5. Employees who are single is most likely to attrite.\n",
    "\n",
    "#    ** Bivariate analysis as hue has to be done for further insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df50d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 11: Model Explainability using SHAP\n",
    "\n",
    "We use **SHAP (SHapley Additive Explanations)** to understand **which features influence attrition predictions**.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.Explainer(model, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`shap.Explainer(model, X_test)`**:  \n",
    "   - Creates a SHAP explainer for the trained model.\n",
    "\n",
    "2. **`shap_values = explainer(X_test)`**:  \n",
    "   - Computes SHAP values for **each feature**.\n",
    "\n",
    "3. **`shap.summary_plot(shap_values, X_test, plot_type=\"bar\")`**:  \n",
    "   - Displays a **bar chart** of feature importance.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Helps **interpret model decisions** (e.g., which features drive attrition predictions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVATION FOR QUANTITATIVE FEATURES HAVING LESS THAN 30 UNIQUE VALUES:\n",
    "\n",
    "# 01. Employees living nearby are more likely to attrite. It has to be further analysed for its controversy.\n",
    "# 02. Employees with education rank 3 & 4 are most likely to attrite.\n",
    "# 03. Employees with 3rd rank jobinvolvement are most likely to attrite.\n",
    "# 04. Employees with joblevel 1 are most likely to attrite.\n",
    "# 05. Employees who have worked only in one company are most likely to attrite.\n",
    "# 06. Employees with less than 14% salary hike are most likely to attrite.\n",
    "# 07. Employees with performance rating of 3 is most likely to attrite.\n",
    "# 08. Employees with 0 stockoptionlevel is most likely to attrite.\n",
    "# 09. Employee with 2 and 3 times of trainingtimelastyear is most likely to attrite.\n",
    "# 10. Employee with rank 3 worklifebalance are most likely to attrite. It has to be analysed for its controversy.\n",
    "# 11. Employee having 1 year of experience in the company is more likely to attrite.\n",
    "# 12. Employee serving in same role for 2 years is most likely to attrite.\n",
    "# 13. Employee serving with 1 and less than 1 year since last promotion is most likely to attrite.\n",
    "# 14. Employee serving with less than 1 year with current manager are most likely to attrite.\n",
    "# 15. Environment, job, relationship satisfication is nearly equally spead in all ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a69e6c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 12: Hyperparameter Tuning - GridSearchCV\n",
    "\n",
    "We optimize the **hyperparameters** using **GridSearchCV**.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "### **Breaking Down the Code:**\n",
    "\n",
    "1. **`param_grid = {'C': [0.01, 0.1, 1, 10, 100]}`**:  \n",
    "   - Defines a **range of values** for the **regularization strength (C)**.\n",
    "\n",
    "2. **`GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')`**:  \n",
    "   - Performs **grid search** over hyperparameters.\n",
    "   - Uses **5-fold cross-validation**.\n",
    "\n",
    "3. **`grid_search.fit(X_train, y_train)`**:  \n",
    "   - Trains multiple models with different parameters.\n",
    "\n",
    "### **Why is This Important?**\n",
    "- Improves model **performance** by selecting the **best hyperparameters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc69ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating plot structure\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "spec = fig.add_gridspec(2,2)\n",
    "spec.update(wspace=0.15,hspace=0.2)\n",
    "sec_1 = fig.add_subplot(spec[0,0])\n",
    "sec_2 = fig.add_subplot(spec[0,1])\n",
    "sec_3 = fig.add_subplot(spec[1,0])\n",
    "sec_4 = fig.add_subplot(spec[1,1])\n",
    "\n",
    "# Adding color preferences\n",
    "bg_color = '#ffd9d9'\n",
    "for selection in [fig, sec_1, sec_2, sec_3, sec_4]:\n",
    "    selection.set_facecolor(bg_color)\n",
    "\n",
    "# Plotting the graph\n",
    "sec = [sec_1, sec_2, sec_3, sec_4]\n",
    "cnt = 0\n",
    "for hue in ['JobLevel', 'PerformanceRating', 'YearsSinceLastPromotion']:\n",
    "    sns.countplot(SalRep_data, x='JobRole', hue=hue, ax=sec[cnt], palette='RdYlBu')\n",
    "    sec[cnt].grid(color='#000000', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "    sec[cnt].set_title('Sales Representative attrition Vs '+hue+' as hue', size=14)\n",
    "    sec[cnt].set_xlabel('')\n",
    "    for location in ['top', 'right', 'left']:\n",
    "        sec[cnt].spines[location].set_visible(False)\n",
    "    cnt+=1\n",
    "\n",
    "# Narrating the observation\n",
    "sec_4.text(0.5,0.6,'OBSERVATION\\n__________________\\n\\n Sales SalRep_data with low JobLevel,\\\n",
    "\\nLowPerformanceRating and with less\\\n",
    "\\nthan 1 YearsSinceLastPromotion are\\n most likely to attrite.',\n",
    "           ha='center',va='center',size=18,weight=550,family='serif')\n",
    "\n",
    "# Removing axis and spines\n",
    "sec_4.xaxis.set_visible(False)\n",
    "sec_4.yaxis.set_visible(False)\n",
    "for location in ['top', 'right', 'left', 'bottom']:\n",
    "    sec_4.spines[location].set_visible(False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
